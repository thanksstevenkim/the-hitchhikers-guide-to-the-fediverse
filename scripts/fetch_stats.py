#!/usr/bin/env python3
"""
Fetch Fediverse instance statistics with ActivityPub verification.

- Incremental save: after EACH instance is processed, results are saved atomically.
- Split outputs:
    * data/stats.ok.json  : verified + sane stats
    * data/stats.bad.json : failed verification, network/parse errors, or anomalous metrics
"""

from __future__ import annotations

import argparse
import json
import logging
import sys
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Sequence, Set, Tuple
from urllib.parse import urlparse, urljoin
import codecs

TIMEOUT = 5
USER_AGENT = "fedlist-stats-fetcher/1.0"
BASE_DIR = Path(__file__).resolve().parent.parent

# Inputs
INSTANCES_PATH = BASE_DIR / "data" / "instances.json"

# Outputs (split)
ALIASES_PATH = BASE_DIR / "data" / "host_aliases.json"
STATS_OK_PATH  = BASE_DIR / "data" / "stats.ok.json"
STATS_BAD_PATH = BASE_DIR / "data" / "stats.bad.json"

# (Legacy) Single-file path retained for compatibility in helper logic
STATS_PATH = BASE_DIR / "data" / "stats.json"

# Network safety limits
MAX_JSON_BYTES = 2_000_000  # 2MB soft cap for JSON payloads
MAX_REDIRECTS = 5
ALLOWED_JSON_CT = ("application/json", "application/ld+json", "application/activity+json")
BLOCKED_SUFFIXES = (".bin", ".zip", ".tar", ".gz", ".xz", ".bz2", ".7z", ".rar", ".mp4", ".mp3", ".avi")

try:  # Optional dependency, falls back to urllib if unavailable
    import requests  # type: ignore
except Exception:  # pragma: no cover - optional import guard
    requests = None


@dataclass
class Instance:
    name: str
    host: str
    url: str
    platform: str


class FetchError(RuntimeError):
    """Raised when a remote fetch fails."""


def _same_host(url: str, host: str) -> bool:
    try:
        h = urlparse(url).hostname
        return (h or "").lower() == host.lower()
    except Exception:
        return False


def _looks_like_binary(url: str) -> bool:
    path = urlparse(url).path.lower()
    return any(path.endswith(suf) for suf in BLOCKED_SUFFIXES)


def _assert_safe_url(url: str, host: str) -> None:
    # ÎèôÏùº Ìò∏Ïä§Ìä∏ ÏïÑÎãàÍ±∞ÎÇò, ÏùòÏã¨ ÌôïÏû•ÏûêÎ©¥ Ï∞®Îã®
    if not _same_host(url, host):
        raise FetchError(f"redirected to different host: {url}")
    if _looks_like_binary(url):
        raise FetchError(f"suspicious path: {url}")

def _is_json_ct(content_type: str) -> bool:
    if not content_type:
        return False
    ct = content_type.split(";")[0].strip().lower()
    # ÌëúÏ§Ä JSON ÎòêÎäî +json ÌååÏÉù ÌÉÄÏûÖ ÌóàÏö©
    return ct == "application/json" or ct.endswith("+json")

def _normalize_host(h: str) -> str:
    # Ìè¨Ìä∏ Ï†úÍ±∞ (IPv4/ÎèÑÎ©îÏù∏ÏóêÎßå Ï†ÅÏö©, IPv6Îäî ÎåÄÍ¥ÑÌò∏ ÌëúÍ∏∞ Í∞ÄÏ†ï)
    raw = (h or "").strip()
    if raw.startswith("["):  # [::1]:8443 ÌòïÌÉúÎäî Í∑∏ÎåÄÎ°ú ÎëêÎêò ÎåÄÍ¥ÑÌò∏ Ï†úÍ±∞Îßå
        # [::1]:8443 ‚Üí ::1]:8443 ‚Üí Í∞ÑÎã® Ï≤òÎ¶¨ Ïñ¥Î†§Ïö∞Î©¥ ÏùºÎã® Ï†ÑÏ≤¥Î•º IDNA Ï≤òÎ¶¨Îßå
        host = raw
    else:
        # ÎèÑÎ©îÏù∏/IPv4: ÎßàÏßÄÎßâ ÏΩúÎ°† Îí§Í∞Ä Ïà´ÏûêÎ©¥ Ìè¨Ìä∏Î°ú Î≥¥Í≥† Ï†úÍ±∞
        if ":" in raw:
            head, tail = raw.rsplit(":", 1)
            if tail.isdigit():
                raw = head
        host = raw
    try:
        return host.encode("idna").decode("ascii").lower().rstrip(".")
    except Exception:
        return host.lower().rstrip(".")


def _same_zone(a: str, b: str) -> bool:
    """
    Í∞ôÏùÄ eTLD+1(ÎåÄÎûµÏ†Å)Ïù∏ÏßÄ ÌåêÏ†ï.
    - Ï†ïÌôïÌïú PSL ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏóÜÏù¥, Ïã§Î¨¥Ïö© Ìú¥Î¶¨Ïä§Ìã±:
      ÏÑúÎ°ú Í∞ôÍ±∞ÎÇò, ÌïúÏ™ΩÏù¥ Îã§Î•∏Ï™ΩÏùò ÏÑúÎ∏åÎèÑÎ©îÏù∏Ïù∏ Í≤ΩÏö∞ ÌóàÏö©.
      (Ïòà: example.org ‚Üî mastodon.example.org)
    """
    if not a or not b:
        return False
    a = _normalize_host(a)
    b = _normalize_host(b)
    if a == b:
        return True
    return a.endswith("." + b) or b.endswith("." + a)

def _assert_safe_url_relaxed(url: str, expected_host: str) -> None:
    """
    'Í∞ôÏùÄ Ï°¥'ÍπåÏßÄ ÌóàÏö©ÌïòÎäî ÏïàÏ†Ñ Í≤ÄÏÇ¨:
    - Îã§Î•∏ Ï°¥ÏúºÎ°úÏùò Î¶¨Îã§Ïù¥Î†âÌä∏/ÎßÅÌÅ¨Îäî Ï∞®Îã®
    - ÏùòÏã¨ ÌôïÏû•Ïûê Ï∞®Îã®ÏùÄ Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ
    """
    parsed = urlparse(url)
    host = (parsed.hostname or "").strip()
    if not _same_zone(host, expected_host):
        raise FetchError(f"redirected to different host: {url}")
    if _looks_like_binary(url):
        raise FetchError(f"suspicious path: {url}")

def _sanitize_charset(enc: Optional[str]) -> str:
    """
    ÏûòÎ™ªÎêú charset Ìó§Îçî Î∞©Ïñ¥:
    - None/ÎπàÍ∞í ‚Üí 'utf-8'
    - ÏΩ§Îßà/Ïä¨ÎûòÏãú/Í≥µÎ∞± Îì± ÏÑûÏù∏ ÎπÑÏ†ïÏÉÅ Í∞í ‚Üí 'utf-8'
    - codecs.lookup Ïã§Ìå® ‚Üí 'utf-8'
    """
    if not enc:
        return "utf-8"
    s = str(enc).strip().strip('"').lower()
    # ÌùîÌïú Ïì∞Î†àÍ∏∞ Ìå®ÌÑ¥ Î∞©Ïñ¥: "utf-8, application/json" Á≠â
    if any(ch in s for ch in (",", "/", " ", "\t", ";")) and s != "utf-8":
        return "utf-8"
    try:
        codecs.lookup(s)
        return s
    except LookupError:
        return "utf-8"

def main() -> None:
    args = parse_args()
    logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

    # --input Ïù¥ ÏûàÏúºÎ©¥ host Î¨∏ÏûêÏó¥/Í∞ùÏ≤¥ Î¶¨Ïä§Ìä∏Î•º, ÏóÜÏúºÎ©¥ instances.jsonÏùÑ ÏÇ¨Ïö©
    if args.input:
        instances = list(load_host_strings(Path(args.input)))
    else:
        instances = list(load_instances(INSTANCES_PATH))

    if not instances:
        logging.error("No instances to process. Populate data/instances.json or pass --input.")
        return

    # ÌòÑÏû¨ UTC ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ (ISO8601, Z)
    now = (
        datetime.now(timezone.utc)
        .replace(microsecond=0)
        .isoformat()
        .replace("+00:00", "Z")
    )

    # Í∏∞Ï°¥ ok/bad ÌååÏùºÏùÑ Í∞ÅÍ∞Å ÎßµÏúºÎ°ú Ï†ÅÏû¨
    ok_map, bad_map = load_existing_stats_maps()

    discovered_hosts: Set[str] = set()
    processed = 0
    updated_ok = 0
    updated_bad = 0

    for instance in instances:
        record, errors, peers = process_instance(instance, now)

        had_errors = bool(errors)
        bucket = classify_record(record, had_errors)  # 'good' or 'bad'

        if bucket == "good":
            prev = ok_map.get(record["host"])
            ok_map[record["host"]] = record
            updated_ok += 1 if (prev is None or prev != record) else 0
            logging.info("OK   %s (%s)", record["host"], record.get("software", {}).get("name") or "-")
        else:
            prev = bad_map.get(record["host"])
            bad_map[record["host"]] = record
            updated_bad += 1 if (prev is None or prev != record) else 0
            reason = "; ".join(errors) if errors else "classified as anomalous/invalid"
            logging.warning("BAD  %s: %s", record["host"], reason)

        processed += 1

        # Ïù∏Ïä§ÌÑ¥Ïä§ ÌïòÎÇò ÎÅùÎÇ† ÎïåÎßàÎã§ Îëê ÌååÏùºÏùÑ ÏõêÏûêÏ†ÅÏúºÎ°ú Ï¶âÏãú Ï†ÄÏû•
        save_stats_pair_atomic(ok_map, bad_map)

        if args.discover_peers and peers:
            discovered_hosts.update(peers)

    logging.info(
        "Incremental save complete: processed=%d, ok_updates=%d, bad_updates=%d",
        processed, updated_ok, updated_bad
    )

    if args.discover_peers:
        # Ïù¥ÎØ∏ Í≤ÄÏÇ¨Ìïú(OK/BAD Îëò Îã§ Ìè¨Ìï®) Ìò∏Ïä§Ìä∏Îäî Ï†úÏô∏
        suggestions = sorted(
            h for h in discovered_hosts if h not in load_checked_hosts()
        )
        emit_peer_suggestions(suggestions, args.peer_output)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Fetch ActivityPub stats (incremental save, split outputs).")
    parser.add_argument(
        "--input",
        help="Input JSON file with host list (plain strings or objects). Results merge into stats.ok/bad.json."
    )
    parser.add_argument(
        "--discover-peers",
        action="store_true",
        help="Attempt to gather federation peers for later curation."
    )
    parser.add_argument(
        "--peer-output",
        default=str(BASE_DIR / "data" / "peer_suggestions.json"),
        help="File path for discovered peers (use '-' for stdout)."
    )
    return parser.parse_args()


# -------------------------------
# Saving & loading (split files)
# -------------------------------

def load_aliases() -> Dict[str, str]:
    """
    ÏõêÎ≥∏Ìò∏Ïä§Ìä∏ -> Ï∫êÎÖ∏ÎãàÏª¨Ìò∏Ïä§Ìä∏ Îß§Ìïë.
    Ïòà: {"0xcb.dev": "mastodon.0xcb.dev"}
    """
    if not ALIASES_PATH.exists():
        return {}
    try:
        data = json.loads(ALIASES_PATH.read_text(encoding="utf-8"))
        if isinstance(data, dict):
            # ÌÇ§/Í∞í Î™®Îëê Ï†ïÍ∑úÌôî
            out: Dict[str, str] = {}
            for k, v in data.items():
                nk = _normalize_host(k)
                nv = _normalize_host(v)
                if nk and nv:
                    out[nk] = nv
            return out
    except Exception:
        pass
    return {}

def save_aliases(aliases: Dict[str, str]) -> None:
    ALIASES_PATH.parent.mkdir(parents=True, exist_ok=True)
    # ÏòàÏÅòÍ≤å Ï†ÄÏû•
    ALIASES_PATH.write_text(
        json.dumps(aliases, indent=2, ensure_ascii=False) + "\n",
        encoding="utf-8"
    )

def register_alias(original_host: str, canonical_host: str) -> None:
    """ÏõêÎ≥∏ ‚Üí Ï∫êÎÖ∏ÎãàÏª¨ Îß§ÌïëÏùÑ Ï∂îÍ∞Ä/Í∞±Ïã†."""
    aliases = load_aliases()
    o = _normalize_host(original_host)
    c = _normalize_host(canonical_host)
    if not o or not c or o == c:
        return
    # ÏÑúÎ°ú Í∞ôÏùÄ Ï°¥Ïùº ÎïåÎßå Í∏∞Î°ù(Î≥¥ÏàòÏ†ÅÏúºÎ°ú)
    if not _same_zone(o, c):
        return
    # Ïù¥ÎØ∏ Í∞ôÏùÄ Í∞íÏù¥Î©¥ Ïä§ÌÇµ
    if aliases.get(o) == c:
        return
    aliases[o] = c
    save_aliases(aliases)

def load_existing_stats_maps() -> Tuple[Dict[str, Dict[str, Any]], Dict[str, Dict[str, Any]]]:
    """
    Load existing OK/BAD stats from split files into host->record maps.
    """
    def _load(path: Path) -> Dict[str, Dict[str, Any]]:
        m: Dict[str, Dict[str, Any]] = {}
        if not path.exists():
            return m
        try:
            data = json.loads(path.read_text(encoding="utf-8"))
            if isinstance(data, list):
                for e in data:
                    if isinstance(e, dict) and "host" in e:
                        m[e["host"]] = e
        except Exception as exc:
            logging.warning("Could not load %s: %s", path.name, exc)
        return m

    ok_map  = _load(STATS_OK_PATH)
    bad_map = _load(STATS_BAD_PATH)

    # (Optional) Legacy single-file import on first run if split files are missing
    if not ok_map and not bad_map and STATS_PATH.exists():
        try:
            data = json.loads(STATS_PATH.read_text(encoding="utf-8"))
            if isinstance(data, list):
                for e in data:
                    if isinstance(e, dict) and "host" in e:
                        # naive split: verified goes to OK, others to BAD
                        (ok_map if e.get("verified_activitypub") else bad_map)[e["host"]] = e
            logging.info("Migrated legacy stats.json into split files (ok/bad).")
        except Exception as exc:
            logging.warning("Could not migrate legacy stats.json: %s", exc)

    return ok_map, bad_map


def save_stats_pair_atomic(ok_map: Dict[str, Dict[str, Any]],
                           bad_map: Dict[str, Dict[str, Any]]) -> None:
    """
    Write OK/BAD lists atomically to their respective files.
    """
    def _write_atomic(path: Path, items: List[Dict[str, Any]]) -> None:
        tmp = path.with_suffix(path.suffix + ".tmp")
        path.parent.mkdir(parents=True, exist_ok=True)
        tmp.write_text(json.dumps(items, indent=2, ensure_ascii=False) + "\n", encoding="utf-8")
        tmp.replace(path)  # atomic on same filesystem

    ok_list  = sorted(ok_map .values(), key=lambda x: x.get("host", ""))
    bad_list = sorted(bad_map.values(), key=lambda x: x.get("host", ""))

    _write_atomic(STATS_OK_PATH,  ok_list)
    _write_atomic(STATS_BAD_PATH, bad_list)


def load_checked_hosts() -> Set[str]:
    checked: Set[str] = set()

    def _merge_from(path: Path) -> None:
        if not path.exists():
            return
        try:
            data = json.loads(path.read_text(encoding="utf-8"))
            if isinstance(data, list):
                for e in data:
                    if isinstance(e, dict):
                        h = e.get("host")
                        if isinstance(h, str) and h:
                            checked.add(_normalize_host(h))
                        rf = e.get("redirected_from")
                        if isinstance(rf, str) and rf:
                            checked.add(_normalize_host(rf))
                        elif isinstance(rf, (list, tuple, set)):
                            for x in rf:
                                if isinstance(x, str) and x:
                                    checked.add(_normalize_host(x))
        except Exception:
            pass

    _merge_from(STATS_OK_PATH)
    _merge_from(STATS_BAD_PATH)
    _merge_from(STATS_PATH)  # legacy

    # Î≥ÑÏπ≠ ÌååÏùºÎèÑ Î≥ëÌï© (ÏõêÎ≥∏ Ìò∏Ïä§Ìä∏Îäî ÏÇ¨Ïã§ÏÉÅ Í≤ÄÏÇ¨Îêú Í≤ÉÏúºÎ°ú Í∞ÑÏ£º)
    for src, dst in load_aliases().items():
        checked.add(_normalize_host(src))
        checked.add(_normalize_host(dst))

    return checked



# -------------------------------
# Classification (good vs bad)
# -------------------------------

def is_anomalous(record: Dict[str, Any]) -> bool:
    """
    Simple anomaly rules:
      - negative counters
      - absurd statuses per user ratio
    """
    u = record.get("users_total")
    s = record.get("statuses")
    am = record.get("users_active_month")

    try:
        if u is not None and u < 0:
            return True
        if s is not None and s < 0:
            return True
        if am is not None and am < 0:
            return True
        if u and s and u > 0 and (s / u) > 50000:
            return True
    except Exception:
        return True
    return False


def classify_record(record: Dict[str, Any], had_errors: bool) -> str:
    """
    Return 'good' or 'bad' based on verification, anomalies, and basic metric presence.
    """
    if not record.get("verified_activitypub"):
        return "bad"
    if had_errors:
        return "bad"
    if is_anomalous(record):
        return "bad"

    # at least one meaningful metric present
    if any(record.get(k) is not None for k in ("users_total", "users_active_month", "statuses")):
        return "good"

    # verified but entirely metric-less -> treat as bad to keep OK file clean
    return "bad"


# -------------------------------
# Loading inputs
# -------------------------------

def load_instances(path: Path) -> Iterable[Instance]:
    if not path.exists():
        logging.error("Instances file not found: %s", path)
        return []

    try:
        data = json.loads(path.read_text(encoding="utf-8"))
    except json.JSONDecodeError as exc:
        logging.error("Invalid JSON in %s: %s", path, exc)
        return []

    if not isinstance(data, list):
        logging.error("Expected a list in %s", path)
        return []
    
    checked_hosts = load_checked_hosts()
    aliases = load_aliases()

    instances: List[Instance] = []
    for entry in data:
        if not isinstance(entry, dict):
            continue
        url = str(entry.get("url", "")).strip()
        if not url:
            logging.warning("Skipping entry without URL: %s", entry)
            continue
        host = extract_host(entry)
        if not host:
            logging.warning("Skipping %s: could not determine host", url)
            continue

        mapped = aliases.get(host, host)
        if mapped in checked_hosts:
            continue

        instances.append(
            Instance(
                name=str(entry.get("name", "")).strip() or mapped,
                host=mapped,
                url=normalize_base_url(url or f"https://{mapped}", mapped),
                platform=str(entry.get("platform", "")).strip().lower() or "unknown",
            )
        )
    return instances


def load_host_strings(path: Path) -> Iterable[Instance]:
    """
    Load a list of hosts given as strings or dict entries.
    Already-checked hosts (in ok/bad or legacy) are skipped automatically.
    """
    if not path.exists():
        logging.error("Host list file not found: %s", path)
        return []

    try:
        data = json.loads(path.read_text(encoding="utf-8"))
    except json.JSONDecodeError as exc:
        logging.error("Invalid JSON in %s: %s", path, exc)
        return []

    if not isinstance(data, list):
        logging.error("Expected a list in %s", path)
        return []

    checked_hosts = load_checked_hosts()
    aliases = load_aliases()
    skipped_count = 0
    instances: List[Instance] = []

    for entry in data:
        if isinstance(entry, str):
            host = _normalize_host(entry)
            if not host:
                continue
            mapped = aliases.get(host, host)
            if mapped in checked_hosts:
                skipped_count += 1
                continue
            instances.append(
                Instance(
                    name=entry.strip() or mapped,
                    host=mapped,
                    url=f"https://{mapped}",
                    platform="unknown",
                )
            )
        elif isinstance(entry, dict):
            url = str(entry.get("url", "")).strip()
            host = extract_host(entry)
            if not host:
                logging.warning("Skipping %s: could not determine host", url)
                continue
            host = _normalize_host(host)

            mapped = aliases.get(host, host)
            if mapped in checked_hosts:
                skipped_count += 1
                continue
            instances.append(
                Instance(
                    name=str(entry.get("name", "")).strip() or mapped,
                    host=mapped,
                    url=normalize_base_url(url or f"https://{mapped}", mapped),
                    platform=str(entry.get("platform", "")).strip().lower() or "unknown",
                )
            )

    logging.info(
        "Loaded %d new hosts from %s (%d already checked, skipped)",
        len(instances), format_relative(path), skipped_count
    )
    return instances


# -------------------------------
# Fetching & parsing
# -------------------------------

def process_instance(instance: Instance, timestamp: str) -> Tuple[Dict[str, Any], List[str], Set[str]]:
    record: Dict[str, Any] = {
        "host": instance.host,
        "verified_activitypub": False,
        "software": {},
        "open_registrations": None,
        "users_total": None,
        "users_active_month": None,
        "statuses": None,
        "languages_detected": [],
        "fetched_at": timestamp,
    }
    errors: List[str] = []
    languages: List[str] = []
    languages_seen = set()
    peers: Set[str] = set()

    canonical_base: Optional[str] = None
    try:
        nodeinfo, canonical_base = fetch_nodeinfo(instance.host)  # ‚Üê ÌäúÌîåÎ°ú Î∞õÏùå
    except FetchError as exc:
        errors.append(f"nodeinfo: {exc}")
        nodeinfo = None

    if nodeinfo:
        record["verified_activitypub"] = True
        update_software(record, nodeinfo.get("software", {}))
        update_open_registrations(record, nodeinfo.get("openRegistrations"))

        usage = nodeinfo.get("usage") if isinstance(nodeinfo, dict) else None
        users = usage.get("users") if isinstance(usage, dict) else None
        update_numeric(record, "users_total", coerce_int(users, "total"))
        update_numeric(record, "users_active_month", coerce_int(users, "activeMonth"))
        update_numeric(record, "statuses", coerce_int(usage, "localPosts"))

        append_languages(languages, languages_seen, usage.get("languages") if isinstance(usage, dict) else None)
        peers.update(extract_peer_hosts_from_nodeinfo(nodeinfo))

    # ‚îÄ‚îÄ Ïó¨Í∏∞ÏÑú base_urlÏùÑ Í≤∞Ï†ï: NodeInfoÍ∞Ä Í∞ÄÎ¶¨ÌÇ® Ï∫êÎÖ∏ÎãàÏª¨ Ïö∞ÏÑ† ‚îÄ‚îÄ
    base_url = canonical_base or instance.url

    # Ï∫êÎÖ∏ÎãàÏª¨ Ìò∏Ïä§Ìä∏Î°ú Î†àÏΩîÎìú host ÏóÖÎç∞Ïù¥Ìä∏ (Í∞ôÏùÄ Ï°¥Ïùº ÎïåÎßå)
    try:
        if canonical_base:
            canon_host = _normalize_host(urlparse(canonical_base).hostname or "")
            if canon_host and _same_zone(canon_host, _normalize_host(instance.host)):
                if canon_host != _normalize_host(instance.host):
                    record["redirected_from"] = instance.host
                    register_alias(instance.host, canon_host)
                record["host"] = canon_host
    except Exception:
        pass

    platform_data: Optional[Dict[str, Any]] = None

    # ÌîåÎû´Ìèº ÏûêÎèô Ï∂îÎ°† (unknown -> software.name)
    platform = instance.platform
    if platform == "unknown" and record.get("software", {}).get("name"):
        detected_name = record["software"]["name"].lower()
        if "mastodon" in detected_name or "hometown" in detected_name or "glitch" in detected_name:
            platform = "mastodon"
        elif "misskey" in detected_name or "calckey" in detected_name or "firefish" in detected_name:
            platform = "misskey"

    if platform == "mastodon":
        try:
            platform_data = fetch_mastodon(base_url)   # ‚Üê Ï∫êÎÖ∏ÎãàÏª¨ base ÏÇ¨Ïö©
        except FetchError as exc:
            errors.append(f"mastodon: {exc}")
        else:
            record["verified_activitypub"] = True
    elif platform == "misskey":
        try:
            platform_data = fetch_misskey(base_url)    # ‚Üê Ï∫êÎÖ∏ÎãàÏª¨ base ÏÇ¨Ïö©
        except FetchError as exc:
            errors.append(f"misskey: {exc}")
        else:
            record["verified_activitypub"] = True
    elif platform != "unknown":
        errors.append(f"unsupported platform: {platform}")

    if platform_data:
        update_software(record, platform_data.get("software", {}))
        update_open_registrations(record, platform_data.get("open_registrations"))
        update_numeric(record, "users_total", platform_data.get("users_total"))
        update_numeric(record, "users_active_month", platform_data.get("users_active_month"))
        update_numeric(record, "statuses", platform_data.get("statuses"))
        append_languages(languages, languages_seen, platform_data.get("languages"))
        peers.update(normalize_peer_list(platform_data.get("peers")))

    record["languages_detected"] = languages
    return record, errors, peers

def extract_peer_hosts_from_nodeinfo(document: Any) -> Set[str]:
    hosts: Set[str] = set()
    if not isinstance(document, dict):
        return hosts
    metadata = document.get("metadata")
    if isinstance(metadata, dict):
        if "peers" in metadata:
            hosts.update(normalize_peer_list(metadata.get("peers")))
        federation = metadata.get("federation")
        if isinstance(federation, dict):
            if "peers" in federation:
                hosts.update(normalize_peer_list(federation.get("peers")))
            if "domains" in federation:
                hosts.update(normalize_peer_list(federation.get("domains")))
    return hosts

def fetch_nodeinfo(host: str) -> Tuple[Dict[str, Any], str]:
    expected = _normalize_host(host)
    last_error: Optional[FetchError] = None
    for scheme in ("https", "http"):
        index_url = f"{scheme}://{expected}/.well-known/nodeinfo"
        try:
            index_payload = request_json(index_url, expected_host=expected)
            if not isinstance(index_payload, dict):
                raise FetchError("unexpected nodeinfo index payload")
            links = index_payload.get("links")
            if not isinstance(links, Sequence):
                raise FetchError("nodeinfo index missing links")
            best_link = select_latest_nodeinfo_link(links)
            if not best_link:
                raise FetchError("no valid nodeinfo links")
            href = best_link.get("href")
            if not isinstance(href, str) or not href:
                raise FetchError("nodeinfo link missing href")

            # Í∞ôÏùÄ Ï°¥ ÌóàÏö© + ÏùòÏã¨ Í≤ΩÎ°ú Ï∞®Îã®
            _assert_safe_url_relaxed(href, expected)

            # Ïù¥ hrefÍ∞Ä Í∞ÄÎ¶¨ÌÇ§Îäî Ìò∏Ïä§Ìä∏/Ïä§ÌÇ¥ÏùÑ 'Ï∫êÎÖ∏ÎãàÏª¨'Î°ú ÏÇ¨Ïö©
            parsed = urlparse(href)
            canon_host = _normalize_host(parsed.hostname or expected)
            canon_scheme = parsed.scheme or "https"
            canon_base = f"{canon_scheme}://{canon_host}"

            payload = request_json(href, expected_host=expected)
            if not isinstance(payload, dict):
                raise FetchError("unexpected nodeinfo document")
            return payload, canon_base
        except FetchError as exc:
            last_error = exc
            continue
    if last_error is not None:
        raise FetchError(str(last_error))
    raise FetchError("nodeinfo endpoint unreachable")

def select_latest_nodeinfo_link(links: Sequence[Any]) -> Optional[Dict[str, Any]]:
    def version_key(link: Dict[str, Any]) -> Tuple[int, int]:
        version = ""
        if isinstance(link, dict):
            rel = link.get("rel")
            href = link.get("href")
            if isinstance(rel, str):
                version = rel.rsplit("/", 1)[-1]
            elif isinstance(href, str):
                version = href.rstrip("/").rsplit("/", 1)[-1]
        major, minor = 0, 0
        if version:
            parts = version.replace("nodeinfo", "").strip("/ ")
            try:
                major_minor = parts.split(".")
                if len(major_minor) >= 2:
                    major = int(major_minor[0])
                    minor = int(major_minor[1])
            except (TypeError, ValueError):
                major, minor = 0, 0
        return major, minor

    candidates = [link for link in links if isinstance(link, dict)]
    if not candidates:
        return None
    return max(candidates, key=version_key)


def fetch_mastodon(base_url: str) -> Dict[str, Any]:
    errors: List[str] = []
    for path in ("/api/v2/instance", "/api/v1/instance"):
        try:
            host = urlparse(base_url).hostname or ""
            payload = request_json(f"{base_url}{path}", expected_host=host)
        except FetchError as exc:
            errors.append(str(exc))
            continue
        if not isinstance(payload, dict):
            continue
        result = parse_mastodon_payload(payload, path.endswith("v2/instance"))
        result["peers"] = sorted(fetch_mastodon_peers(base_url))
        return result
    raise FetchError("; ".join(errors) if errors else "instance API unavailable")


def fetch_mastodon_peers(base_url: str) -> Set[str]:
    try:
        host = urlparse(base_url).hostname or ""
        payload = request_json(f"{base_url}/api/v1/instance/peers", expected_host=host)
    except FetchError:
        return set()
    return normalize_peer_list(payload)


def parse_mastodon_payload(payload: Dict[str, Any], is_v2: bool) -> Dict[str, Any]:
    usage = payload.get("usage") if isinstance(payload, dict) else None
    users = usage.get("users") if isinstance(usage, dict) else None
    stats = payload.get("stats") if isinstance(payload, dict) else None
    configuration = payload.get("configuration") if isinstance(payload, dict) else None

    result: Dict[str, Any] = {
        "software": {
            "name": payload.get("software", {}).get("name")
            if isinstance(payload.get("software"), dict)
            else payload.get("version") and "mastodon",
            "version": payload.get("version"),
        },
        "open_registrations": payload.get("registrations", {}).get("enabled")
        if isinstance(payload.get("registrations"), dict)
        else payload.get("registrations"),
        "users_total": first_int(
            coerce_int(users, "total"),
            coerce_int(stats, "user_count"),
        ),
        "users_active_month": first_int(
            coerce_int(users, "activeMonth"),
            coerce_int(stats, "active_month"),
        ),
        "statuses": first_int(
            coerce_int(usage, "localPosts"),
            coerce_int(stats, "status_count"),
        ),
        "languages": [],
    }

    lang_seen: set = set()
    if configuration and isinstance(configuration, dict):
        append_languages(result["languages"], lang_seen, configuration.get("languages"))
    elif is_v2:
        append_languages(result["languages"], lang_seen, payload.get("languages"))

    software = payload.get("software")
    if isinstance(software, dict):
        result["software"] = {
            "name": software.get("name"),
            "version": software.get("version"),
        }

    return result


def fetch_misskey(base_url: str) -> Dict[str, Any]:
    host = urlparse(base_url).hostname or ""
    payload = request_json(f"{base_url}/api/meta", method="POST", json_body={"detail": True}, expected_host=host)
    if not isinstance(payload, dict):
        raise FetchError("unexpected meta payload")

    stats = payload.get("stats") if isinstance(payload, dict) else None

    result: Dict[str, Any] = {
        "software": {
            "name": payload.get("softwareName") or "misskey",
            "version": payload.get("version"),
        },
        "open_registrations": payload.get("disableRegistration") is False,
        "users_total": first_int(
            coerce_int(stats, "originalUsersCount"),
            coerce_int(stats, "usersCount"),
        ),
        "users_active_month": first_int(
            coerce_int(stats, "monthlyActiveUsers"),
            coerce_int(stats, "activeUsers"),
        ),
        "statuses": first_int(
            coerce_int(stats, "originalNotesCount"),
            coerce_int(stats, "notesCount"),
        ),
        "languages": [],
    }

    federation = payload.get("federation") if isinstance(payload, dict) else None
    if isinstance(federation, dict):
        result["peers"] = sorted(normalize_peer_list(federation.get("peers")))
    return result


# -------------------------------
# Utilities
# -------------------------------

def emit_peer_suggestions(hosts: Sequence[str], target: str) -> None:
    """
    Save newly discovered peers, excluding ones already checked (ok/bad/legacy).
    """
    if not hosts:
        logging.info("No federation peers discovered.")
        return

    checked_hosts = load_checked_hosts()
    new_hosts = [h for h in hosts if h not in checked_hosts]

    if not new_hosts:
        logging.info("All discovered peers already checked.")
        return

    if target == "-":
        json.dump(new_hosts, sys.stdout, ensure_ascii=False, indent=2)
        sys.stdout.write("\n")
        logging.info("Emitted %d peers to stdout.", len(new_hosts))
        return

    path = Path(target)
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(new_hosts, indent=2, ensure_ascii=False) + "\n", encoding="utf-8")
    logging.info("Wrote %s (%d new peers).", format_relative(path), len(new_hosts))


def format_relative(path: Path) -> str:
    try:
        return str(path.relative_to(BASE_DIR))
    except ValueError:
        return str(path)


def extract_host(entry: Dict[str, Any]) -> str:
    host = str(entry.get("host", "")).strip().lower()
    if host:
        return _normalize_host(host)

    url = entry.get("url")
    if isinstance(url, str) and url:
        parsed = urlparse(url)
        if parsed.hostname:
            return _normalize_host(parsed.hostname)
        return _normalize_host(url.strip().rstrip("/"))
    return ""


def normalize_base_url(url: str, host: str) -> str:
    if not url:
        return f"https://{host}"
    parsed = urlparse(url)
    scheme = parsed.scheme or "https"
    netloc = parsed.netloc or host
    path = parsed.path.rstrip("/")
    if not path:
        path = ""
    rebuilt = f"{scheme}://{netloc}{path}"
    return rebuilt.rstrip("/")


def update_software(record: Dict[str, Any], software: Any) -> None:
    if not isinstance(software, dict):
        return
    target = record.get("software")
    if not isinstance(target, dict):
        target = {}
        record["software"] = target
    name = software.get("name")
    version = software.get("version")
    if name and not target.get("name"):
        target["name"] = str(name)
    if version and not target.get("version"):
        target["version"] = str(version)


def update_open_registrations(record: Dict[str, Any], value: Any) -> None:
    boolean = coerce_bool(value)
    if boolean is None:
        return
    if record.get("open_registrations") is None:
        record["open_registrations"] = boolean


def update_numeric(record: Dict[str, Any], key: str, value: Any) -> None:
    number = coerce_int_value(value)
    if number is None:
        return
    if record.get(key) is None:
        record[key] = number


def append_languages(target: List[str], seen: set, values: Any) -> None:
    if isinstance(values, dict):
        values = values.values()
    if isinstance(values, (str, bytes)):
        values = [values]
    if not isinstance(values, Sequence) and not isinstance(values, set):
        return
    for value in values:
        code = normalize_language_code(value)
        if not code:
            continue
        if code in seen:
            continue
        seen.add(code)
        target.append(code)


def normalize_peer_list(values: Any) -> Set[str]:
    hosts: Set[str] = set()
    if values is None:
        return hosts
    if isinstance(values, dict):
        for item in values.values():
            hosts.update(normalize_peer_list(item))
        return hosts
    if isinstance(values, (list, tuple, set)):
        for item in values:
            hosts.update(normalize_peer_list(item))
        return hosts
    host = normalize_peer_host(values)
    if host:
        hosts.add(host)
    return hosts


def normalize_peer_host(value: Any) -> Optional[str]:
    if value is None:
        return None
    text = str(value).strip()
    if not text:
        return None
    text = text.rstrip("/")
    if text.startswith("http://") or text.startswith("https://"):
        parsed = urlparse(text)
        if parsed.hostname:
            host = parsed.hostname.lower()
            if parsed.port:
                return f"{host}:{parsed.port}"
            return host
        text = text.split("://", 1)[-1]
    return text.lower()


def normalize_language_code(value: Any) -> Optional[str]:
    if value is None:
        return None
    text = str(value).strip()
    if not text:
        return None
    return text.lower()


def first_int(*values: Any) -> Optional[int]:
    for value in values:
        number = coerce_int_value(value)
        if number is not None:
            return number
    return None


def coerce_int(mapping: Any, key: str) -> Optional[int]:
    if isinstance(mapping, dict):
        return coerce_int_value(mapping.get(key))
    return None


def coerce_int_value(value: Any) -> Optional[int]:
    if value is None:
        return None
    try:
        number = int(value)
    except (TypeError, ValueError):
        return None
    if number < 0:
        return None
    return number


def coerce_bool(value: Any) -> Optional[bool]:
    if isinstance(value, bool):
        return value
    if value in {"true", "True", "1", 1}:
        return True
    if value in {"false", "False", "0", 0}:
        return False
    return None


def request_json(
    url: str,
    method: str = "GET",
    json_body: Optional[Dict[str, Any]] = None,
    expected_host: Optional[str] = None,
) -> Any:
    """
    ÏïàÏ†ÑÌïú JSON ÌéòÏπò:
      - Content-Type Í≤ÄÏ¶ù (application/*json)
      - ÏµúÎåÄ Î∞îÏù¥Ìä∏ Ï†úÌïú (MAX_JSON_BYTES)
      - ÎèôÏùº Ìò∏Ïä§Ìä∏ Î¶¨Îã§Ïù¥Î†âÌä∏Îßå ÌóàÏö©, MAX_REDIRECTS Ï†úÌïú
      - Î∞îÏù¥ÎÑàÎ¶¨ ÏùòÏã¨ Í≤ΩÎ°ú Ï∞®Îã®
      - 4xx/5xx ÏÉÅÌÉúÏΩîÎìúÎäî FetchErrorÎ°ú Î≥ÄÌôò
    """
    headers = {
        "Accept": "application/json, */*+json; q=0.9",
        "User-Agent": USER_AGENT,
    }

    if expected_host:
        _assert_safe_url_relaxed(url, expected_host)

    if requests is not None:
    # ----- requests Î≤ÑÏ†Ñ -----
        import requests as _req
        session = _req.Session()
        session.max_redirects = MAX_REDIRECTS

        class _SameHostAdapter(_req.adapters.HTTPAdapter):
            def build_response(self, req, resp):
                r = super().build_response(req, resp)
                if r.is_redirect:
                    loc = r.headers.get("location")
                    if loc:
                        next_url = urljoin(r.url, loc)
                        if expected_host:
                            _assert_safe_url_relaxed(next_url, expected_host)
                return r

        adapter = _SameHostAdapter(max_retries=0)
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        def _do(method: str, url: str, data: Optional[Dict[str, Any]]):
            try:
                resp = session.request(
                    method,
                    url,
                    json=data,
                    timeout=TIMEOUT,
                    headers=headers,
                    stream=True,        # Ïä§Ìä∏Î¶¨Î∞ç
                    allow_redirects=True,
                )
            except _req.exceptions.RequestException as e:
                # ‚úÖ DNS Ïã§Ìå®/Ïó∞Í≤∞ Ïã§Ìå®/ÌÉÄÏûÑÏïÑÏõÉ Îì± Î™®Îì† ÎÑ§Ìä∏ÏõåÌÅ¨ ÏòàÏô∏Î•º FetchErrorÎ°ú Î≥ÄÌôò
                raise FetchError(str(e))
                # üîê ÏÉÅÌÉúÏΩîÎìú ÏßÅÏ†ë Í≤ÄÏÇ¨ (HTTPErrorÎ°ú ÌÑ∞ÏßÄÏßÄ ÏïäÍ≤å)
            status = getattr(resp, "status_code", None)
            if status is None or status >= 400:
                raise FetchError(f"HTTP {status or 'unknown'} from {url}")
                # Content-Type ÌôïÏù∏
            ct = (resp.headers.get("Content-Type") or "")
            if not _is_json_ct(ct):
                raise FetchError(f"unexpected Content-Type: {ct or 'unknown'}")

            # Content-Length ÏÑ†Í≤ÄÏÇ¨
            clen = resp.headers.get("Content-Length")
            if clen is not None:
                try:
                    if int(clen) > MAX_JSON_BYTES:
                        raise FetchError(f"payload too large: {clen} bytes")
                except ValueError:
                    pass

                # Î≥∏Î¨∏ Ï†úÌïú ÏùΩÍ∏∞
            buf = bytearray()
            for chunk in resp.iter_content(chunk_size=65536):
                if chunk:
                    buf.extend(chunk)
                    if len(buf) > MAX_JSON_BYTES:
                        raise FetchError(f"payload exceeded {MAX_JSON_BYTES} bytes limit")
            enc = _sanitize_charset(getattr(resp, "encoding", None))
            text = buf.decode(enc, errors="replace")

            try:
                return json.loads(text)
            except json.JSONDecodeError as exc:
                raise FetchError(f"Invalid JSON response from {url}: {exc}")

        return _do(method, url, json_body)

    # ----- urllib Î≤ÑÏ†Ñ -----
    import urllib.error
    import urllib.request

    data_bytes: Optional[bytes] = None
    req_headers = headers.copy()
    if json_body is not None:
        req_headers["Content-Type"] = "application/json"
        data_bytes = json.dumps(json_body).encode("utf-8")

    # ÏàòÎèô Î¶¨Îã§Ïù¥Î†âÌä∏ Ï≤òÎ¶¨(ÎèôÏùº Ìò∏Ïä§Ìä∏Îßå)
    current_url = url
    for _ in range(MAX_REDIRECTS + 1):
        if expected_host:
            _assert_safe_url_relaxed(current_url, expected_host)

        request = urllib.request.Request(current_url, data=data_bytes, headers=req_headers, method=method)
        try:
            with urllib.request.urlopen(request, timeout=TIMEOUT) as resp:
                # Î¶¨Îã§Ïù¥Î†âÌä∏ Ï≤òÎ¶¨
                if 300 <= resp.status < 400:
                    loc = resp.headers.get("Location")
                    if not loc:
                        raise FetchError(f"redirect without location from {current_url}")
                    next_url = urljoin(current_url, loc)
                    if expected_host:
                        _assert_safe_url_relaxed(next_url, expected_host)
                    current_url = next_url
                    # Îã§Ïùå Î£®ÌîÑÎ°ú (Î¶¨Îã§Ïù¥Î†âÌä∏ hop)
                    continue

                # üîê ÏÉÅÌÉúÏΩîÎìú Í≤ÄÏÇ¨
                if resp.status >= 400:
                    raise FetchError(f"HTTP {resp.status} from {current_url}")

                # Content-Type Í≤ÄÏÇ¨
                ct = resp.headers.get("Content-Type") or ""
                if not _is_json_ct(ct):
                    raise FetchError(f"unexpected Content-Type: {ct or 'unknown'}")

                # Content-Length ÏÑ†Í≤ÄÏÇ¨
                clen = resp.headers.get("Content-Length")
                if clen is not None:
                    try:
                        if int(clen) > MAX_JSON_BYTES:
                            raise FetchError(f"payload too large: {clen} bytes")
                    except ValueError:
                        pass

                # Ï†úÌïú ÏùΩÍ∏∞
                buf = bytearray()
                while True:
                    chunk = resp.read(65536)
                    if not chunk:
                        break
                    buf.extend(chunk)
                    if len(buf) > MAX_JSON_BYTES:
                        raise FetchError(f"payload exceeded {MAX_JSON_BYTES} bytes limit")
                enc = _sanitize_charset(resp.headers.get_content_charset())
                text = buf.decode(enc, errors="replace")

        except urllib.error.URLError as exc:
            raise FetchError(str(exc))

        try:
            return json.loads(text)
        except json.JSONDecodeError as exc:
            raise FetchError(f"Invalid JSON response from {current_url}: {exc}")

    raise FetchError("too many redirects")


if __name__ == "__main__":
    main()
